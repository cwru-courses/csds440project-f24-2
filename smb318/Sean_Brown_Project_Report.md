## Survey

## Methods
For my project, I implemented the tri training algorithm, which is a semi-supervised learning technique that leverages multiple classifiers to improve the performance of a model. In many applications, unlabeled training examples are much more available than labeled ones. As a result, co-training is a popular learning algorithm. In co-training, two classifiers (C1 and C2) are trained on two distinct, complementary views of a dataset (often split by their features). After the classifiers are trained on a labeled dataset, each classifier labels a portion of the unlabeled data. The most confident predictions of C1 are added to the training set of C2, and vice versa. The classifiers are retrained iteratively until the labeled set no longer changes.
 
While co-training has proven to be useful, it has its limitations. The strongest of these is that the algorithm requires two sufficient and redundant views. In other words, the sets of attributes used by each classifier must contain all the information needed to predict the target attribute, and the attribute sets must be conditionally independent from each other given the class label. If the sufficient and redundant view assumption is not met, co-training will not perform well. Co-training can also suffer from pseudo-labeling, and it strongly depends on the quality of C1 and C2. If one classifier makes systematic errors, these errors can propagate to the other classifier as more points are labeled.
 
Tri-training aims to address these limitations. In tri-training, three classifiers (C1, C2, and C3) are initially trained on the labeled data. They use the same base learning algorithm, so diversity in the classifiers is obtained by training them on different bootstrap samples of the labeled data. Next, the classifiers are iteratively refined using unlabeled examples. Under certain conditions, an unlabeled example is labeled for a classifier if the other two agree on the label. For example, C1 and C2 label samples for C3. Then, each classifier is retrained using the original labeled data and the newly labeled samples. The labeling and retraining process repeats until the labeled set stops changing. Once the classifiers are finalized, predictions on new data are made using a majority vote among the three classifiers.
 
Crucially, tri-training does not require sufficient and redundant views. Diversity among the classifiers is necessary so that the algorithm does not turn into self-training, but since we use different bootstrap samples for each initial classifier, sufficient and redundant views are not needed to create diversity. This is vital, as such views may be unavailable or extremely difficult to create. Using three classifiers instead of two also creates advantages. Greater diversity is obtained, and pseudo-labels are less likely to propagate. While co-training only needs one classifier to label a point incorrectly, tri-training requires two classifiers to make a joint misclassification in order for a pseudo-label to propagate. Similarly, using three classifiers reduces errors once the classifiers are finalized, as a majority vote is used for prediction.

## Research
While tri-training creates diversity by using bootstrap sampling, I felt that simply using bootstrap sampling leaves something to be desired in this regard. Diversity is critical for ensemble learning methods, since diverse models tend to make different errors. Ideally, we would like each classifier to make independent errors to maximize overall accuracy, minimize overfitting to noise, and improve the robustness of our final model. So, my idea for the research extension was to run tri-training using multiple different base learners (decision trees, Naive Bayes classifiers, etc.) for each of the 3 initial classifiers. For example, we could use one decision tree classifier, one naive Bayes classifier, and one logistic regression classifier. Surprisingly, no research has been done in this area. All existing implementations of tri-training use the same base learning algorithm for all three initial classifiers.
 
My idea was that using different base learners can take advantage of each of their strengths and mitigate each other’s weaknesses. For example, decision trees can capture complex relationships, and handle both numerical and categorical data. However, they are prone to overfitting and struggle with interactions. Similarly, Naive Bayes is simple, fast, and generally effective, but its assumption of feature independence given the class label does not always hold.

So, for my research extension, I modified the tri-training algorithm to allow different base learners. I will call this new algorithm modifiedTriTraining. The modified algorithm no longer includes bootstrap sampling to train the initial classifiers, instead training them on the entire labeled dataset. This is another advantage of my extension - we are able to train on a (slightly) greater variety of data without requiring any more labeled examples. I chose 4 different combinations of base learners, as listed below - the models in each combination complement each other.

1. decision trees, logistic regression, k nearest neighbors
2. SVM, naive Bayes, decision tree
3. SVM, decision tree, logistic regression
4. decision tree, k nearest neighbors, naive Bayes

## Results, Analysis, Discussion

To evaluate my extension’s performance, I used an approach to similar to that of Zhou et al. in the original tri-training paper. I compared the performance of tri-training and modifiedTriTraining on the “australian” and “ionosphere” datasets from Zhou et al.’s paper. Zhou et al. keep 25% of the data as test examples, while the rest are used as the pool of training examples (i.e. L ∪ U, where L and U are the labeled and unlabeled examples, respectively). The authors evaluate tri-training at “unlabel rates” of 80%, 60%, 40%, and 20%. “Australian” and “ionosphere” are both fully labeled datasets, so the “unlabel rate” is simply the percentage of examples whose labels we choose not to use, mimicking a supervised learning problem.
  
Zhou et al. evaluate tri-training using the error rate on the test sets. To ensure that my results are replicable, I set the random_state to 42 when I split the datasets into training and test sets. However, tri-training still involves randomness in the bootstrap sampling. So, to get a better sense of the actual error rate, I ran tri-training 10 times for each dataset, and computed the mean error. For both datasets, Zhou et al. showed that using decision trees as the base learner for tri-training gave the lowest error rate in nearly all cases. So, I used decision trees as the base learner in my experiment. Next, I ran modifiedTriTraining once for each combination of base learners, and computed the resulting errors. modifiedTriTraining does not use bootstrap sampling, so no repetition was necessary. The errors were computed for unlabel rates 80%, 60%, 40%, and 20% on both datasets.

Below is a plot of the results for the Australia dataset.

## Bibliography

